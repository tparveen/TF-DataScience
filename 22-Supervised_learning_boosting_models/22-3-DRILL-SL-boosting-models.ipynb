{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradiant Boosting Challenge\n",
    "### Whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradiant Decent algorithm : Throughout all iterations of the gradient descent algorithm for linear regression, one thing remains constant: The underlying data used to estimate the parameters and calculate the loss function never changes. \n",
    "\n",
    "* In gradient boosting, however, the underlying data do change. \n",
    "\n",
    "* Most often,gradient boosting uses decision trees, and minimizes either the residual (regression trees) or the negative log-likelihood (classification trees).\n",
    "\n",
    "* Each time we run a decision tree, we extract the residuals. Then we run a new decision tree, using those residuals as the outcome to be predicted. After reaching a stopping point, we add together the predicted values from all of the decision trees to create the final gradient boosted prediction.\n",
    "\n",
    "* We'll calculate a tree, store the predicted values, pull the residuals, and run a new tree on the residuals. This will repeat 101 times. At the end, we add together all the predicted values from each iteration to yield the final predictions.\n",
    "\n",
    "* the more iterations we run, the more likely we are to overfit. Gradient boost comes with some methods to avoid overfitting. Cross-validation will check for overfitting, but there are also methods that can be applied before using the test set that will reduce the likelihood of overfit.\n",
    "\n",
    "* One option is subsampling, where each iteration of the boost algorithm uses a subsample of the original data. By introducing some randomness into the process, subsampling makes it harder to overfit.\n",
    "\n",
    "* Another option is shrinkage, which we have encountered before in ridge regression. Here, the shrinkage/regularization parameter reduces the impact of subsequent iterations on the final solution.\n",
    "\n",
    "\n",
    "* sklearn packages \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ### Example: Whether or not someone lives with a partner.\n",
    " - this will be a categorical outcome:\n",
    " - this will be a classifier. Since we're now working with a binary outcome, we've switched to a classifier. Now our loss function can't be the residuals. Our options are \"deviance\", or \"exponential\". Deviance is used for logistic regression, and we'll try that here.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8147 entries, 0 to 8593\n",
      "Data columns (total 13 columns):\n",
      "cntry      8147 non-null object\n",
      "idno       8147 non-null float64\n",
      "year       8147 non-null int64\n",
      "tvtot      8147 non-null float64\n",
      "ppltrst    8147 non-null float64\n",
      "pplfair    8147 non-null float64\n",
      "pplhlp     8147 non-null float64\n",
      "happy      8147 non-null float64\n",
      "sclmeet    8147 non-null float64\n",
      "sclact     8147 non-null float64\n",
      "gndr       8147 non-null float64\n",
      "agea       8147 non-null float64\n",
      "partner    8147 non-null float64\n",
      "dtypes: float64(11), int64(1), object(1)\n",
      "memory usage: 891.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cntry</th>\n",
       "      <th>idno</th>\n",
       "      <th>year</th>\n",
       "      <th>tvtot</th>\n",
       "      <th>ppltrst</th>\n",
       "      <th>pplfair</th>\n",
       "      <th>pplhlp</th>\n",
       "      <th>happy</th>\n",
       "      <th>sclmeet</th>\n",
       "      <th>sclact</th>\n",
       "      <th>gndr</th>\n",
       "      <th>agea</th>\n",
       "      <th>partner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CH</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CH</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CH</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CH</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CH</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CH</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CH</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CH</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CH</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CH</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cntry  idno  year  tvtot  ppltrst  pplfair  pplhlp  happy  sclmeet  sclact  \\\n",
       "0     CH   5.0     6    3.0      3.0     10.0     5.0    8.0      5.0     4.0   \n",
       "1     CH  25.0     6    6.0      5.0      7.0     5.0    9.0      3.0     2.0   \n",
       "2     CH  26.0     6    1.0      8.0      8.0     8.0    7.0      6.0     3.0   \n",
       "3     CH  28.0     6    4.0      6.0      6.0     7.0   10.0      6.0     2.0   \n",
       "4     CH  29.0     6    5.0      6.0      7.0     5.0    8.0      7.0     2.0   \n",
       "6     CH  40.0     6    3.0      0.0      5.0     2.0    0.0      2.0     2.0   \n",
       "7     CH  41.0     6    2.0      4.0      5.0     3.0   10.0      5.0     2.0   \n",
       "8     CH  51.0     6    2.0      8.0      8.0     8.0    9.0      6.0     4.0   \n",
       "9     CH  53.0     6    4.0      4.0      4.0     8.0    7.0      4.0     2.0   \n",
       "10    CH  55.0     6    1.0      6.0      7.0     7.0    9.0      5.0     2.0   \n",
       "\n",
       "    gndr  agea  partner  \n",
       "0    2.0  60.0      1.0  \n",
       "1    2.0  59.0      1.0  \n",
       "2    1.0  24.0      2.0  \n",
       "3    2.0  64.0      1.0  \n",
       "4    2.0  55.0      1.0  \n",
       "6    1.0  76.0      1.0  \n",
       "7    2.0  30.0      1.0  \n",
       "8    2.0  84.0      2.0  \n",
       "9    2.0  62.0      1.0  \n",
       "10   2.0  33.0      1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1 \n",
    "\n",
    "### \n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8147, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.9)   ### setting 90% of the data shape(row,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params) ## what are the ** for?\n",
    "clf.fit(X_train, y_train) ## fit the model\n",
    "\n",
    "predict_train = clf.predict(X_train) ### make prediction\n",
    "predict_test = clf.predict(X_test)   ### test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04650845608292417\n",
      "Percent Type II errors: 0.17607746863066012\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06257668711656442\n",
      "Percent Type II errors: 0.18527607361963191\n"
     ]
    }
   ],
   "source": [
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It appears that age and happiness are the most important features in predicting whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Improve this gradient boost model\n",
    " \n",
    "* While this model is already doing alright, we've seen from the Type I and Type II error rates that there is definitely room for improvement. Your task is to see how low you can get the error rates to go in the test set, based on your model in the training set. \n",
    "\n",
    "Right now the errors are looking:\n",
    "\n",
    "Training set accuracy:\n",
    "Percent Type I errors: 0.04650845608292417\n",
    "Percent Type II errors: 0.17607746863066012\n",
    "\n",
    "Test set accuracy:\n",
    "Percent Type I errors: 0.06257668711656442\n",
    "Percent Type II errors: 0.18527607361963191\n",
    "\n",
    "Strategies you might use include:\n",
    "\n",
    "- Creating new features\n",
    "- Applying more overfitting-prevention strategies like subsampling\n",
    "- More iterations\n",
    "- Trying a different loss function\n",
    "- Changing the structure of the weak learner: Allowing more leaves in the tree, or other modifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
