{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26-6-Gaussian Mixture Models (GMM) approach to clustering\n",
    "\n",
    "So far in the module, we reviewed the algorithms that assign observations to only a single cluster. This type of clustering algorithms are called hard clustering. There exists another type of clustering algorithms such that each observation is assigned to several clusters with associated probabilities. This strand of clustering algorithms is called soft clustering. In this checkpoint, we present a soft clustering algorithm called Gaussian Mixture Models (in short GMM) which belongs to a general class of probabilistic clustering algorithms.\n",
    "\n",
    "The main advantages of GMM are as follows:\n",
    "\n",
    "It's a soft clustering algorithm. So, we can assess the confidence of the cluster assignments by investigating the probabilities.\n",
    "It doesn't assume anything about the geometry of the clusters unlike k-means. So, it can also tackle with the non-linear geometries.\n",
    "\n",
    "The assumption of our data being generated by a mix of normal distributions may sound too strong. But, if you recall the Central Limit Theorem, it states that if we have enough samples from a population, the means of the samples converge to a normal distribution no matter the original distribution of the population. Counting on this theorem, GMM searches for the means and the standard deviations of the Gaussian (normal) distributions.\n",
    "\n",
    "#### Assumptions of GMM\n",
    "\n",
    "There are two important assumptions that GMM makes:\n",
    "The first one is that there are k distributions that generate the data. In effect, this is equivalent to say that there are exactly k clusters in the data.\n",
    "The other assumption is that all of these k distributions are Gaussians. However, GMM doesn't put constraints on the parameters of these Gaussians but estimates them such that the likelihood of the data being generated by these k Gaussians is maximized.\n",
    "\n",
    "GMM might become an expensive algorithm in terms of computational time. Hence, applying it to very high-dimensional datasets may take too long to converge. When we have very high-dimensional datasets, we may consider applying a dimensionality reduction technique first to reduce the dimension of the data before applying GMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "* Apply GMM to the heart disease data by setting n_components=2. \n",
    "* Get ARI and silhoutte scores for your solution and compare it with those of the k-means and hierarchical clustering solutions that you implemented in the assignments of the previous checkpoints. \n",
    "* Which algorithm does perform better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import metrics\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "postgres_db = 'heartdisease'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
    "heartdisease_df = pd.read_sql_query('select * from heartdisease',con=engine)\n",
    "\n",
    "# no need for an open connection, as we're only doing a single query\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and the outcome\n",
    "X = heartdisease_df.iloc[:, :13]\n",
    "y = heartdisease_df.iloc[:, 13]\n",
    "\n",
    "# Replace missing values (marked by ?) with a 0\n",
    "X = X.replace(to_replace='?', value=0)\n",
    "\n",
    "# Binarize y so that 1 means heart disease diagnosis and 0 means no diagnosis.\n",
    "y = np.where(y > 0, 0, 1)\n",
    "\n",
    "# Standardize the data.\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI score: 0.18389186035089963\n",
      "Silhouette score: 0.13628813153331445\n"
     ]
    }
   ],
   "source": [
    "# Defining the agglomerative clustering\n",
    "gmm_cluster = GaussianMixture(n_components=2, random_state=123)\n",
    "\n",
    "# Fit model\n",
    "clusters = gmm_cluster.fit_predict(X_std)\n",
    "\n",
    "print(\"ARI score: {}\".format(\n",
    "    metrics.adjusted_rand_score(y, clusters)))\n",
    "\n",
    "print(\"Silhouette score: {}\".format(\n",
    "    metrics.silhouette_score(X_std, clusters, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM scores lower than both k-means and hierarchical clustering in terms of ARI and silhouette scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GMM implementation of scikit-learn has a parameter called covariance_type. This parameter determines the type of covariance parameters to use. Specifically, there are four types you can specify:\n",
    "- full: This is the default. Each component has its own general covariance matrix.\n",
    "- tied: All components share the same general covariance matrix.\n",
    "- diag: Each component has its own diagonal covariance matrix.\n",
    "- spherical: Each component has its own single variance.\n",
    "\n",
    "Try all of these. Which one does perform better in terms of ARI and silhouette scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI score with covariance_type=full: 0.18389186035089963\n",
      "Silhouette score with covariance_type=full: 0.13628813153331445\n",
      "------------------------------------------------------\n",
      "ARI score with covariance_type=tied: 0.18389186035089963\n",
      "Silhouette score with covariance_type=tied: 0.13628813153331445\n",
      "------------------------------------------------------\n",
      "ARI score with covariance_type=diag: 0.18389186035089963\n",
      "Silhouette score with covariance_type=diag: 0.13628813153331445\n",
      "------------------------------------------------------\n",
      "ARI score with covariance_type=spherical: 0.20765243525722465\n",
      "Silhouette score with covariance_type=spherical: 0.12468753110276873\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Defining the agglomerative clustering\n",
    "gmm_cluster = GaussianMixture(n_components=2, random_state=123, covariance_type=\"full\")\n",
    "\n",
    "# Fit model\n",
    "clusters = gmm_cluster.fit_predict(X_std)\n",
    "\n",
    "print(\"ARI score with covariance_type=full: {}\".format(\n",
    "    metrics.adjusted_rand_score(y, clusters)))\n",
    "\n",
    "print(\"Silhouette score with covariance_type=full: {}\".format(\n",
    "    metrics.silhouette_score(X_std, clusters, metric='euclidean')))\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "# Defining the agglomerative clustering\n",
    "gmm_cluster = GaussianMixture(n_components=2, random_state=123, covariance_type=\"tied\")\n",
    "\n",
    "# Fit model\n",
    "clusters = gmm_cluster.fit_predict(X_std)\n",
    "\n",
    "print(\"ARI score with covariance_type=tied: {}\".format(\n",
    "    metrics.adjusted_rand_score(y, clusters)))\n",
    "\n",
    "print(\"Silhouette score with covariance_type=tied: {}\".format(\n",
    "    metrics.silhouette_score(X_std, clusters, metric='euclidean')))\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "# Defining the agglomerative clustering\n",
    "gmm_cluster = GaussianMixture(n_components=2, random_state=123, covariance_type=\"diag\")\n",
    "\n",
    "# Fit model\n",
    "clusters = gmm_cluster.fit_predict(X_std)\n",
    "\n",
    "print(\"ARI score with covariance_type=diag: {}\".format(\n",
    "    metrics.adjusted_rand_score(y, clusters)))\n",
    "\n",
    "print(\"Silhouette score with covariance_type=diag: {}\".format(\n",
    "    metrics.silhouette_score(X_std, clusters, metric='euclidean')))\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# Defining the agglomerative clustering\n",
    "gmm_cluster = GaussianMixture(n_components=2, random_state=123, covariance_type=\"spherical\")\n",
    "\n",
    "# Fit model\n",
    "clusters = gmm_cluster.fit_predict(X_std)\n",
    "\n",
    "print(\"ARI score with covariance_type=spherical: {}\".format(\n",
    "    metrics.adjusted_rand_score(y, clusters)))\n",
    "\n",
    "print(\"Silhouette score with covariance_type=spherical: {}\".format(\n",
    "    metrics.silhouette_score(X_std, clusters, metric='euclidean')))\n",
    "print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARI score of covariance type spherical is higher than the others and its silhouette score is lower than the others. The scores of the other covariance types are the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
